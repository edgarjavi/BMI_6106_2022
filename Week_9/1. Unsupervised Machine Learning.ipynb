{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Unsupervised Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unsupervised Machine learning is a series of techniques where the main goal is to reduce the dimensionality of the data. In unsupervised learning there are not outcome variables (y variable) only input variables (X).\n",
    "\n",
    "Another important goal is to understand the underlying structure of the data which in turns will help to understand the data itself.\n",
    "\n",
    "In here we will focus on Clustering Algorithms as there are widely use in multiple fields in statistics (not only in machine learning approaches), and because there are the starting point on data modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![title](machine-learning-cheet-sheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Normally, unsupervised learning uses all of the data, not just a subset (training data) as the ultimate goal is to reduce dimensionality and understand the underlying behavior of the data.\n",
    "\n",
    "[https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $A.$ Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principal component analysis the main goal is to use algebraic properties of high dimensionality data and reduce it to fewer dimensions but still maintaining the variance signal among the variables.\n",
    "\n",
    "Principal component is one technique from the dimensionality reduction family. PCA has been extensively used due to its great flexibility, and general applicability. \n",
    "\n",
    "Here is a extensive figure of the multiple techniques for dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](MachineLearningAlgorithms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations in a study, research or project there are many variables that carry little or not information or association with the purpose of the study. In other cases, there is high redundancy that makes difficult to parse and make sense of the overall structure of the data. There is also variable duplication and variables with error or outliers that make the data difficult to treat. It is generally a good practice to remove these variables to reduce the amount of noise they carry. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a technique from a family of dimensionality reduction techniques called *singular value decomposition* (SVD). The basic idea for SVD is to rotate the coordinate axes in order to maximize the underlying variability of the model using a small number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic steps for a PCA are:\n",
    "\n",
    "1. Generate a covariance matrix with all of the variables present in the model\n",
    "    1a. If variables come from different levels of measurements, standarized all variables.\n",
    "2. Calculate eigenvalues of the covariance matrix\n",
    "3. Using the eigenvector matrix obtain variance explained on each component of the matrix\n",
    "4. Order the variance from highest to lowest (principal components)\n",
    "5. Calculate vector directionalty on each component for all variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In PCA and other Factor analysis the source of variation we will be measuring relies on the variance and covariance rather than the mean as we have seen in parametric methods. \n",
    "\n",
    "    There are two main types of variance that we will be using on this analysis \n",
    "    \n",
    "    Common variance: shared variance across a set of items (highly correlated variables would carry high variance)\n",
    "    \n",
    "    Unique Variance: Portions of the variance that are not common between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Eigenvalues and eigenvector are the main variation units that will allow you to measure how much each variable contribute to the overall variation within your data. The idea is to decompose the common variance (in PCA, however other types of factor analysis will use the unique variance as well) into vectors in a multidimentional space.\n",
    "\n",
    "When you have multiple variables you need to decompose the variation of each variable into linear components to be able to compare across variables, to do this you do linear transformation of each variable variation. Eigenvectors and eigenvalues are the resulting vectors from such linear transformation. \n",
    "\n",
    "The number of components to evaluate correspond to the number of variables you have in your analysis, and each the dimmension of the that decomposed vector matrix will be equal to the number of variables to evaluate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explanation is taken from (https://stats.idre.ucla.edu/spss/seminars/efa-spss/)[https://stats.idre.ucla.edu/spss/seminars/efa-spss/]\n",
    "\n",
    "<b>Eigenvalues represent the total amount of variance that can be explained by a given principal component.</b>  They can be positive or negative in theory, but in practice they explain variance which is always positive.\n",
    "\n",
    "If eigenvalues are greater than zero, then itâ€™s a good sign.\n",
    "Since variance cannot be negative, negative eigenvalues imply the model is ill-conditioned.\n",
    "Eigenvalues close to zero imply there is item multicollinearity, since all the variance can be taken up by the first component.\n",
    "\n",
    "Eigenvalues are also the sum of squared component loadings across all variables for each component, which represent the amount of variance in each variables that can be explained by the principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Eigenvectors represent a weight for each eigenvalue</b>. The eigenvector times the square root of the eigenvalue gives the component loadings which can be interpreted as the correlation of each item with the principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a better understanding we will follow an example given by the UC Business Analytics R Programming Guide [http://uc-r.github.io/pca](http://uc-r.github.io/pca) \n",
    "And if we have time we will follow an case example to better understand the practical applications of the unsupervised learning techniques.\n",
    "\n",
    "\n",
    "We will be using the dataset USAarrests that contains four variables that represent the number of arrests per 100,000 residents for Assault, Murder, and Rape in each of the fifty US states in 1973. The data set also contains the percentage of the population living in urban areas, UrbanPop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)  # data manipulation and visualization\n",
    "library(gridExtra)  # plot arrangement\n",
    "\n",
    "data(\"USArrests\")\n",
    "#glimpse(USArrests)\n",
    "head(USArrests, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lets standardize each variable to avoid inflation from variable with different units of measurement\n",
    "\n",
    "apply(USArrests, 2, var)\n",
    "scaled_df <- apply(USArrests, 2, scale)\n",
    "head(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?scale\n",
    "##Remember variable standardization using z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before the goal of a PCA is to reduce the number of dimensions in the data structure by maximixing the variance on each linear combination of variables and ranking on these combinations from higher variance to lower variance.\n",
    "\n",
    "The first component is the one that include the largest variance across all features. The second principal component, contains the second largest variance and that is uncorrelated with PC1. \n",
    "\n",
    "The elements for each PC are notated as:\n",
    "\n",
    "$Z1 = \\theta_{11}X_1 + \\theta_{21}X_2 + ... + \\theta_{p1}X_p  $\n",
    "\n",
    "where all $\\theta$ are the loadings of the principal components (something similar to coefficients but in an aggregate value across all features)\n",
    "\n",
    "We need to calculate the loadings vector that maximizes the variance. To achieve this we calculate the eigenvector from the largest eigenvalue of the covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eigenvalues & eigenvectors\n",
    "arrests.cov <- cov(scaled_df)\n",
    "arrests.eigen <- eigen(arrests.cov)\n",
    "str(arrests.eigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values correspond to the eigenvalues for each component and the vectors the loadings (eigenvectors for each component and for each feature)\n",
    "\n",
    "We can extract the loadings for the first two Principal components (PC1 and PC2). By default R gives the sign of the loading in the negative direction, depending of the type of data we might want to change it to a positive direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(phi <- arrests.eigen$vectors[,1:2])\n",
    "phi <- -phi\n",
    "row.names(phi) <- c(\"Murder\", \"Assault\", \"UrbanPop\", \"Rape\")\n",
    "colnames(phi) <- c(\"PC1\", \"PC2\")\n",
    "phi ##What do these values mean??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember principal components are unrelated across, which means that each component is their own feature (variable space).\n",
    "\n",
    "If we project the n data points x1,x2...xn onto the first eigenvector, the projected values are called the principal component scores for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC1 <- as.matrix(scaled_df) %*% phi[,1]\n",
    "PC2 <- as.matrix(scaled_df) %*% phi[,2]\n",
    "\n",
    "# Create data frame with Principal Components scores\n",
    "PC <- data.frame(State = row.names(USArrests), PC1, PC2)\n",
    "head(PC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot now the scores on the first two principal components in order to better analyze the patterns given on each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "# Plot Principal Components for each State\n",
    "ggplot(PC, aes(PC1, PC2)) + \n",
    "  modelr::geom_ref_line(h = 0) +\n",
    "  modelr::geom_ref_line(v = 0) +\n",
    "  geom_text(aes(label = State), size = 3) +\n",
    "  xlab(\"First Principal Component\") + \n",
    "  ylab(\"Second Principal Component\") + \n",
    "  ggtitle(\"First Two Principal Components of USArrests Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After an initial inspection of the data we need to define how many components we need to extract to account for most of the variation of the data.\n",
    "\n",
    "We can the proportion of the variation from each component by dividing each eigenvalue by the total number of principal components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PVE <- arrests.eigen$values / sum(arrests.eigen$values)\n",
    "round(PVE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PVE (aka scree) plot\n",
    "PVEplot <- qplot(c(1:4), PVE) + \n",
    "  geom_line() + \n",
    "  xlab(\"Principal Component\") + \n",
    "  ylab(\"PVE\") +\n",
    "  ggtitle(\"Scree Plot\") +\n",
    "  ylim(0, 1)\n",
    "\n",
    "# Cumulative PVE plot\n",
    "cumPVE <- qplot(c(1:4), cumsum(PVE)) + \n",
    "  geom_line() + \n",
    "  xlab(\"Principal Component\") + \n",
    "  ylab(NULL) + \n",
    "  ggtitle(\"Cumulative Scree Plot\") +\n",
    "  ylim(0,1)\n",
    "\n",
    "grid.arrange(PVEplot, cumPVE, ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R there are many packages that can be used to run PCA that generate very nice visuals and more information than given in here. You can continue the tutorial given in [http://uc-r.github.io/pca](http://uc-r.github.io/pca) that uses the function prcomp from the base package.\n",
    "\n",
    "Other packages that have PCA analysis are (see [http://www.gastonsanchez.com/visually-enforced/how-to/2012/06/17/PCA-in-R/](http://www.gastonsanchez.com/visually-enforced/how-to/2012/06/17/PCA-in-R/) for more details:\n",
    "\n",
    "-  prcomp() (stats)\n",
    "-  princomp() (stats)\n",
    "-  PCA() (FactoMineR)\n",
    "-  dudi.pca() (ade4)\n",
    "-  acp() (amap)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
