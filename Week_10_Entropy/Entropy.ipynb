{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information theory\n",
    "\n",
    "The primary goal of information theory is to quantify how much information is in data. We can evaluate the amount of information generated by a random sample by evaluating its frequency in light of a prior knowledge of the distribution of such random sample. \n",
    "\n",
    "There are multiple ways to obtaing the amount of information from a random variable that differ on the type of information we want to obtain, we will review three of the most used measurements in information theory: Entropy, KL divergence and Mutual Information.\n",
    "\n",
    "We will examine simple cases where the random events are independent, discrete and mutually exclusive. As you might have guess we will be evaluating random events that come from bernoulli trials.\n",
    "\n",
    "Remember that a discrete random Event:\n",
    "\n",
    "$$ H = \\sum_{k=1}^{L} P(k) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "The most important metric in information theory is called Entropy, typically denoted as H. The definition of Entropy for a probability distribution is given by the Shannon entropy which is a special derivation of the Hartley function [https://en.wikipedia.org/wiki/Hartley_function](https://en.wikipedia.org/wiki/Hartley_function):\n",
    "\n",
    "$$ H = - \\sum_{i=1}^{N} P(X_i) * log p(X_i) $$\n",
    "\n",
    "If we use $log_2$ for our calculation we can interpret entropy as \"the minimum number of bits it would take us to encode our information\".\n",
    "\n",
    "Entropy is usually defined as a measurement of disorder (ambiguity, uncertainty), where low entropy will be given to random samples where the element do not convey any information regarding the sample. On the other hand, when events carry low probability values, these events carry more information thus the amount of entropy increases.\n",
    "\n",
    "![Entropy_A](Entropy_A.png)\n",
    "\n",
    "Entropy is normally measured in bits, the units of measurement depends on the base of the logarithm used but normally the log 2 is frequently used.\n",
    "\n",
    "For example.\n",
    "\n",
    "if we throw a fair coin the Entropy of this event is 1, as each value is equally probable. If a value is certainty to occur then entropy is 0 (random varriable is deterministic) as the value does not provide any information regarding its outcome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-(1/2 * log(1/2,2)+ 1/2 *log(1/2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0 1 2 3 4 \n",
       "3 2 3 1 2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "y1\n",
       "0 1 2 3 4 \n",
       "9 1 1 3 3 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = c(4, 2, 3, 0, 2, 4, 0, 0, 2, 1, 1)\n",
    "y1 = c(0,0,0,0,0,0,0,0,0,1,2,3,3,3,4,4,4)\n",
    "\n",
    "table(y)\n",
    "table(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.23127025460758"
      ],
      "text/latex": [
       "2.23127025460758"
      ],
      "text/markdown": [
       "2.23127025460758"
      ],
      "text/plain": [
       "[1] 2.23127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1.84986872258518"
      ],
      "text/latex": [
       "1.84986872258518"
      ],
      "text/markdown": [
       "1.84986872258518"
      ],
      "text/plain": [
       "[1] 1.849869"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freqs <- table(y)/length(y)\n",
    "-sum(freqs * log2(freqs))\n",
    "freqs <- table(y1)/length(y1)\n",
    "-sum(freqs * log2(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2.23127025460758"
      ],
      "text/latex": [
       "2.23127025460758"
      ],
      "text/markdown": [
       "2.23127025460758"
      ],
      "text/plain": [
       "[1] 2.23127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1.84986872258518"
      ],
      "text/latex": [
       "1.84986872258518"
      ],
      "text/markdown": [
       "1.84986872258518"
      ],
      "text/plain": [
       "[1] 1.849869"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(DescTools)\n",
    "\n",
    "Entropy(table(y))\n",
    "Entropy(table(y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure the amount of entropy for each letter on the English alphabet by the English alphabet, if we assume that all characters (26 letters and a space) are equally likely then:\n",
    "\n",
    "h = -(log1/27) = 4.75\n",
    "\n",
    "but this prior of the distribution is not correct, in regular English some characters occur more frequent than others.\n",
    "\n",
    "![English_Entropy](English_Entropy.png)\n",
    "\n",
    "In this case the Entropy is 4.219 bits per symbol, that the value is smaller indicates that there are some characters that occur much more frequently than other therefore there is less information on these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Entropy\n",
    "\n",
    "When we want to compare the uncertainty between a set of random variables we can calculate the joint entropy by the equation:\n",
    "\n",
    "$$ H(X,Y) = - \\sum_{S_X}\\sum_{S_Y}p(x,y)\\log p(x,y) $$\n",
    "\n",
    "[http://www.cs.tau.ac.il/~iftachh/Courses/Info/Fall14/Printouts/Lesson2_h.pdf](http://www.cs.tau.ac.il/~iftachh/Courses/Info/Fall14/Printouts/Lesson2_h.pdf)\n",
    "\n",
    "[https://stats.stackexchange.com/questions/72694/joint-entropy-of-two-random-variables](https://stats.stackexchange.com/questions/72694/joint-entropy-of-two-random-variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL-Divergence\n",
    "\n",
    "Kullback-Leibler Divergence (also known as relative entropy)\n",
    "\n",
    "If P(x) and Q(x) are two continuous probability density functions, then the Kullback-Leibler divergence of q from p is defined as \n",
    "\n",
    "\n",
    "$$ \\mbox{KL}(p~||~q)\n",
    "= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} $$\n",
    "\n",
    "\n",
    "is a non-symmetric measure of difference between two probability distributions. It is related to mutual information and can be used to measure the association between two random variables. It measures the average number of extra bits that the random sample x diverge from y. In other words: The KL divergence between p and q can also be seen as the average number of bits that are wasted by encoding events from a distribution p with a code based on a not-quite-right distribution q.\n",
    "\n",
    "![Entropy_KL](Entropy_KL.png)\n",
    "\n",
    "Very often in Probability and Statistics we'll replace observed data or a complex distributions with a simpler, approximating distribution. KL Divergence helps us to measure just how much information we lose when we choose an approximation.\n",
    "\n",
    "For a good example see [https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information\n",
    "\n",
    "If entropy is how much uncertainty there is in a random sample, mutual information pertains how much information is shared between two random variables, or how much information from one random variable depends from another random variable. Even though mutual information value is related to the correlation coefficient, it is also linked to the joint distribution of both random samples. The base 2 logarithm is often used, in which case result is in units of bits.\n",
    "\n",
    "$$I(x,y) = \\sum_{x,y} P(x,y) \\ln {{P(x,y)}\\over{P(x) P(y)}}$$\n",
    "\n",
    "Simple Estimator\n",
    "Let $(X_i, Y_i)$ be a data set of $N$ data points. To compute the empirical mutual information, count the frequencies of each combination of values, $freq(x,y)$, and then compute:\n",
    "\n",
    "$freq(x) = \\sum_y freq(x,y)$\n",
    "\n",
    "$freq(y) = \\sum_x freq(x,y)$\n",
    "\n",
    "$I_N(x,y) = \\sum_{x,y} freq(x,y) \\ln {{freq(x,y)}\\over{freq(x) freq(y)}}$\n",
    "\n",
    "when computing the sum, only terms where freq(x,y)>0.\n",
    "\n",
    "[http://www.lumina.com/blog/estimation-of-mutual-information](http://www.lumina.com/blog/estimation-of-mutual-information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
