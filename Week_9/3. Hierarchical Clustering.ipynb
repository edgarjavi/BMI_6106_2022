{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $3.$ Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering is an alternative to k-means methods in which allows for a rapid visualization of the clustering dynamics of the data using multiples attributes of the relationship across each observation.\n",
    "\n",
    "This technique is widely use (in some fields even more used than k-means and other clustering algorithms) for example in the biological sciences. Similar to k-means, in hierarchical clustering there is no need to partition the data in train and test sets but we can use the power of all of the data to correctly present the hierarchical structure of the variables.\n",
    "\n",
    "In contrast to k-means we don't need to specify the number of clusters a priori, but we can obtain a visual representation of the cluster in the for of a dendogram\n",
    "\n",
    "![title](dendro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hierarchical clustering will be presented using the same example we have been using for unsupervised learning thanks to the tutorial provided by UC Business Analytics R Programming Guide [http://uc-r.github.io/hc_clustering](http://uc-r.github.io/hc_clustering) We will continue working with the USAarrests dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)  # data manipulation\n",
    "library(cluster)    # clustering algorithms\n",
    "library(factoextra) # clustering visualization\n",
    "library(dendextend) # for comparing two dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The algorithm for hierarchical clustering can be enumerate as:\n",
    "\n",
    "1. compute similarity matrix across elements of the matrix\n",
    "2. compute cluster similarity across pairs of clusters\n",
    "3. Each element is considered as a single cluster (root or bottom of tree depending on technique)\n",
    "4. At the next step the cluster is divided or combined based on cluster similarity across elements of the cluster\n",
    "5. Normally the split is a dichotomous split, however, there can be polytomies in some cases (especially in low variance elements)\n",
    "6. Plot dendogram and cut at particular levels based on intrinsic characteristics of each cluster.\n",
    "\n",
    "Two main techniques for hierarchical clustering AGNES (agglomerative clustering a bottom-up approach) and DIANA (Divisive hierarchical clustering a Top-down method)\n",
    "\n",
    "We already reviewed some methods for computing similarity matrices. To compute cluster similarities we can use linkage methods that attempt to compute average (or other estimate) pairwise distances between elements of a cluster and the next cluser.\n",
    "\n",
    "The most common methods are: complete linkage, single linkage, average linkage, centroid linkage, Ward's minimum distance.\n",
    "\n",
    "![title](dendro_clusters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's prepare the data\n",
    "\n",
    "df <- USArrests\n",
    "df <- na.omit(df)\n",
    "df <- scale(df)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To generate the hierarchical clustering first we create the distance matrix using the euclidean method, then we calculate the clustering distances using the function hclust and the method complete (compere to ward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissimilarity matrix\n",
    "d <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# Hierarchical clustering using Complete Linkage\n",
    "hc1 <- hclust(d, method = \"complete\" )\n",
    "\n",
    "# Plot the obtained dendrogram\n",
    "plot(hc1, cex = 0.6, hang = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We can also use the AGNES function. With this function you can only produce an agglomerative coefficient (measures how much cluster structure there is in the data - values closer to 1 indicate a strong clustering structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute with agnes\n",
    "hc2 <- agnes(df, method = \"complete\")\n",
    "\n",
    "# Agglomerative coefficient\n",
    "hc2$ac\n",
    "## [1] 0.8531583"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Which is the best clustering method (there is no right or wrong answer but some help to maximize the strongest clustering coefficient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to assess\n",
    "m <- c( \"average\", \"single\", \"complete\", \"ward\")\n",
    "names(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n",
    "\n",
    "# function to compute coefficient\n",
    "ac <- function(x) {\n",
    "  agnes(df, method = x)$ac\n",
    "}\n",
    "\n",
    "map_dbl(m, ac)\n",
    "##   average    single  complete      ward \n",
    "## 0.7379371 0.6276128 0.8531583 0.9346210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc3 <- agnes(df, method = \"ward\")\n",
    "pltree(hc3, cex = 0.6, hang = -1, main = \"Dendrogram of agnes\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We can also use the function DIANA from the cluster package, however, for this clustering algorithm there is no clustering methods available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute divisive hierarchical clustering\n",
    "hc4 <- diana(df)\n",
    "\n",
    "# Divise coefficient; amount of clustering structure found\n",
    "hc4$dc\n",
    "## [1] 0.8514345\n",
    "\n",
    "# plot dendrogram\n",
    "pltree(hc4, cex = 0.6, hang = -1, main = \"Dendrogram of diana\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The notation for dendograms starts with the leaf that is each observation, branches are a combination of similar leaves which are combined further high on the tree until reaching the root.\n",
    "\n",
    "Similar to k-means, We can cut our dendogram at a particular branch height which determines the number of clusters to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ward's method\n",
    "hc5 <- hclust(d, method = \"ward.D2\" )\n",
    "\n",
    "# Cut tree into 4 groups\n",
    "sub_grp <- cutree(hc5, k = 4)\n",
    "\n",
    "# Number of members in each cluster\n",
    "table(sub_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We can add the cluster identity to the original matrix\n",
    "USArrests %>%\n",
    "  mutate(cluster = sub_grp) %>%\n",
    "  head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For better visualization we can generate borders that separate each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hc5, cex = 0.6)\n",
    "rect.hclust(hc5, k = 4, border = 2:5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_cluster(list(data = df, cluster = sub_grp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also compare two dendrograms that were obtained using different hierarchical clusterings to compare the new and stable branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance matrix\n",
    "res.dist <- dist(df, method = \"euclidean\")\n",
    "\n",
    "# Compute 2 hierarchical clusterings\n",
    "hc1 <- hclust(res.dist, method = \"complete\")\n",
    "hc2 <- hclust(res.dist, method = \"ward.D2\")\n",
    "\n",
    "# Create two dendrograms\n",
    "dend1 <- as.dendrogram (hc1)\n",
    "dend2 <- as.dendrogram (hc2)\n",
    "\n",
    "tanglegram(dend1, dend2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function entanglement produces a measurement of how similar the trees are (good alingment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dend_list <- dendlist(dend1, dend2)\n",
    "\n",
    "tanglegram(dend1, dend2,\n",
    "  highlight_distinct_edges = FALSE, # Turn-off dashed lines\n",
    "  common_subtrees_color_lines = FALSE, # Turn-off line colors\n",
    "  common_subtrees_color_branches = TRUE, # Color common branches \n",
    "  main = paste(\"entanglement =\", round(entanglement(dend_list), 2))\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We also need to determine the optimal number of clusters to cut\n",
    "\n",
    "Similar to k-means we can use the elbow method to generate a scree plot and visualy inspect the best number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_nbclust(df, FUN = hcut, method = \"wss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
